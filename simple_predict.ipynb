{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "81d841d1-748f-4f93-850d-cb84bf8e211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "from iteround import saferound\n",
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "82f77f67-41bd-402d-b9e0-8eb92721af2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([[1,2],[1,4]]).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7424e126-cbac-43af-a6ff-64e91abcf041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76bbbd6e-8683-496e-9e53-1b3848293d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_FOLDER = \"./dataset\"\n",
    "train_processed_data = os.path.join(DATASET_FOLDER, 'processed/sku_feature_data.parquet')\n",
    "test_data = os.path.join(DATASET_FOLDER, 'test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "174e81dc-0a45-42ca-baca-f5698393dccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_v1 = pd.read_parquet(train_processed_data, engine='fastparquet')\n",
    "df_train_v1 = df_train_v1.set_index('sku')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8ddad92-7164-4c9c-9071-cae6532f2334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_domain_id                            object\n",
       "item_id                                    int64\n",
       "item_title                                object\n",
       "site_id                                   object\n",
       "product_id                                object\n",
       "product_family_id                         object\n",
       "count                                      int64\n",
       "date_first                        datetime64[ns]\n",
       "date_last                         datetime64[ns]\n",
       "date_diff                                  int64\n",
       "sold_quantity_first                        int64\n",
       "sold_quantity_last                         int64\n",
       "sold_quantity_sum                          int64\n",
       "sold_quantity_mean                       float64\n",
       "sold_quantity_std                        float64\n",
       "sold_quantity_min                          int64\n",
       "sold_quantity_max                          int64\n",
       "sold_quantity_mode                         int64\n",
       "sold_quantity_mode_tx                    float64\n",
       "current_price_first                      float64\n",
       "current_price_last                       float64\n",
       "current_price_sum                        float64\n",
       "current_price_mean                       float64\n",
       "current_price_std                        float64\n",
       "current_price_min                        float64\n",
       "current_price_max                        float64\n",
       "current_price_mode                       float64\n",
       "current_price_mode_tx                    float64\n",
       "minutes_active_first                     float64\n",
       "minutes_active_last                      float64\n",
       "minutes_active_sum                       float64\n",
       "minutes_active_mean                      float64\n",
       "minutes_active_std                       float64\n",
       "minutes_active_min                       float64\n",
       "minutes_active_max                       float64\n",
       "minutes_active_mode                      float64\n",
       "minutes_active_mode_tx                   float64\n",
       "currency_first                          category\n",
       "currency_last                           category\n",
       "currency_mode                           category\n",
       "currency_mode_tx                         float64\n",
       "listing_type_first                      category\n",
       "listing_type_last                       category\n",
       "listing_type_mode                       category\n",
       "listing_type_mode_tx                     float64\n",
       "shipping_logistic_type_first            category\n",
       "shipping_logistic_type_last             category\n",
       "shipping_logistic_type_mode             category\n",
       "shipping_logistic_type_mode_tx           float64\n",
       "shipping_payment_first                  category\n",
       "shipping_payment_last                   category\n",
       "shipping_payment_mode                   category\n",
       "shipping_payment_mode_tx                 float64\n",
       "sold_quantity_series                      object\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_v1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edebe205-3bc4-46be-82a8-7cfc1a0c84a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "3f84a551-96f2-4305-af34-d03f98f4c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#uniform\n",
    "predictions=np.empty((len(df_test), 30))\n",
    "predictions.fill(0.0333)\n",
    "predictions[:, -1] = 0.0343\n",
    "\n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/uniform.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "cb0c3b86-733f-4ba0-bd32-aaa1695fc1e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbd76f8380184484b31a83cde5975fdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#random uniform\n",
    "random_uniform = np.random.uniform(size=(len(df_test), 30))\n",
    "\n",
    "def normalize_f(data_point):\n",
    "    data = data_point/data_point.sum()\n",
    "    rounded = saferound(data, places=4)\n",
    "    return np.array(rounded)\n",
    "    \n",
    "predictions = []\n",
    "with Pool(50) as p:\n",
    "    for data in tqdm(p.imap(normalize_f, random_uniform), total=len(random_uniform)):\n",
    "        predictions.append(data)\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/random.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d3b3560d-94d9-45ee-a45b-7e83034df000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2729cd43abc24694a128714be53cd5be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#random cummulative\n",
    "random_uniform = np.random.uniform(size=(len(df_test), 30))\n",
    "\n",
    "def cumm_f(data_point):\n",
    "    data = np.cumsum(data_point)\n",
    "    data = data/data.sum()\n",
    "    rounded = saferound(data, places=4)\n",
    "    return np.array(rounded)\n",
    "    \n",
    "predictions = []\n",
    "with Pool(50) as p:\n",
    "    for data in tqdm(p.imap(cumm_f, random_uniform), total=len(random_uniform)):\n",
    "        predictions.append(data)\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/random_cummulative.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "f7d31edf-fc73-4e80-9f77-1e7bd3582c0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37270b36268346ec999ea6a00a8cc68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#random specific date\n",
    "random_int = np.random.randint(0, 30, size=len(df_test))\n",
    "\n",
    "def uniform_after_date(specific_date):\n",
    "    data = np.zeros(30)\n",
    "    data[specific_date:].fill(1/(30-specific_date))\n",
    "    rounded = saferound(data, places=4)\n",
    "    return np.array(rounded)\n",
    "    \n",
    "predictions = []\n",
    "with Pool(100) as p:\n",
    "    for data in tqdm(p.imap(uniform_after_date, random_int), total=len(random_int)):\n",
    "        predictions.append(data)\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/uniform_after_random_specific_date.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f449db4a-3ebf-4de0-ad66-f48b5fdd4d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spike specific date\n",
    "predictions = np.eye(30)[np.random.choice(30, len(df_test))]\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/spike_random_specific_date.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "db2812c2-addd-487b-a59d-009c3f30bc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46b8ed0495f6419d937edb2aab21518f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simple_first_30_days(data):\n",
    "    i, row = data\n",
    "    sku = row['sku']\n",
    "    target_stock = row['target_stock']\n",
    "    train_row = df_train_v1.loc[sku]\n",
    "    sold_quantity_series = json.loads(train_row['sold_quantity_series'])\n",
    "    if len(sold_quantity_series) < 30:\n",
    "        sold_quantity_series = np.pad(sold_quantity_series, 30-len(sold_quantity_series))\n",
    "\n",
    "    sold_quantity_series = sold_quantity_series[:30]\n",
    "\n",
    "    sold_quantity_cumsum = np.cumsum(sold_quantity_series)\n",
    "    stock_percentage = sold_quantity_cumsum/target_stock\n",
    "    stock_percentage = np.clip(stock_percentage,0,1)\n",
    "\n",
    "    if stock_percentage[-1] == 0:\n",
    "        stock_percentage[-1] = 1\n",
    "\n",
    "    probalities = stock_percentage/stock_percentage.sum()\n",
    "    probalities = saferound(probalities, places=4)\n",
    "    probalities = np.array(probalities)\n",
    "    return (sku, probalities)\n",
    "\n",
    "predictions = []\n",
    "skus = []\n",
    "with Pool(100) as p:\n",
    "    for data in tqdm(p.imap(simple_first_30_days, df_test.iterrows()), total=len(df_test)):\n",
    "        sku, probabilities = data\n",
    "        skus.append(sku)\n",
    "        predictions.append(probabilities)\n",
    "        \n",
    "skus = np.array(skus)\n",
    "comparison = skus == df_test['sku'].to_numpy()\n",
    "assert comparison.all()\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/simple_first_30_days.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4bbb7fdd-9e47-4c11-aab7-4b1ffff4a2fe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df5c610fb2734ec48ea1626e2c6911b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def simple_first_30_days_fixed_spike(data):\n",
    "    #row = df_test.iloc[0]\n",
    "    i, row = data\n",
    "    sku = row['sku']\n",
    "    target_stock = row['target_stock']\n",
    "    train_row = df_train_v1.loc[sku]\n",
    "    sold_quantity_series = json.loads(train_row['sold_quantity_series'])\n",
    "\n",
    "    original_len = len(sold_quantity_series)\n",
    "\n",
    "    if original_len < 30:\n",
    "        sold_quantity_series = np.pad(sold_quantity_series, (0, 30-original_len))\n",
    "\n",
    "    sold_quantity_series = sold_quantity_series[:30]\n",
    "\n",
    "    sold_quantity_cumsum = np.cumsum(sold_quantity_series)\n",
    "    stock_percentage = sold_quantity_cumsum/target_stock\n",
    "    stock_percentage = np.clip(stock_percentage,0,1)\n",
    "\n",
    "    index_max = np.argmax(stock_percentage == stock_percentage.max())\n",
    "    probalities = np.eye(30)[index_max]\n",
    "    return (sku, probalities)\n",
    "\n",
    "predictions = []\n",
    "skus = []\n",
    "with Pool(100) as p:\n",
    "    for data in tqdm(p.imap(simple_first_30_days_fixed_spike, df_test.iterrows()), total=len(df_test)):\n",
    "        sku, probabilities = data\n",
    "        skus.append(sku)\n",
    "        predictions.append(probabilities)\n",
    "        \n",
    "skus = np.array(skus)\n",
    "comparison = skus == df_test['sku'].to_numpy()\n",
    "assert comparison.all()\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/simple_first_30_days_fixed_spike.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9134a7b5-3682-4736-9a40-d6290fde1b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e24e9285f904df987911421196f431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shift(arr, num, fill_value=0):\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = fill_value\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = fill_value\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result[:] = arr\n",
    "    return result\n",
    "\n",
    "def voted_shifted_padded_spike(data):\n",
    "    i, row = data\n",
    "    sku = row['sku']\n",
    "    target_stock = row['target_stock']\n",
    "    train_row = df_train_v1.loc[sku]\n",
    "    sold_quantity_series = json.loads(train_row['sold_quantity_series'])\n",
    "\n",
    "    original_len = len(sold_quantity_series)\n",
    "\n",
    "    if original_len < 59:\n",
    "        sold_quantity_series = np.pad(sold_quantity_series, (0, 59-original_len))\n",
    "\n",
    "    #sold_quantity_series = sold_quantity_series[:30]\n",
    "    voted_probalities = np.zeros(30)\n",
    "    for i in range(original_len):\n",
    "        shifted_sum_sold_quantity_series = shift(sold_quantity_series, -i)[:30]\n",
    "\n",
    "        sold_quantity_cumsum = np.cumsum(shifted_sum_sold_quantity_series)\n",
    "        stock_percentage = sold_quantity_cumsum/target_stock\n",
    "        stock_percentage = np.clip(stock_percentage,0,1) \n",
    "        max_consumed = stock_percentage.max()\n",
    "\n",
    "        index_max = np.argmax(stock_percentage == max_consumed)\n",
    "        shifted_probalities = np.eye(30)[index_max]\n",
    "        shifted_probalities *= max_consumed\n",
    "        voted_probalities += shifted_probalities\n",
    "\n",
    "    index_max = np.argmax(voted_probalities == voted_probalities.max())\n",
    "    probalities = np.eye(30)[index_max]\n",
    "\n",
    "    #if (voted_probalities == np.zeros(30)).all():\n",
    "    #    voted_probalities[0] = 1\n",
    "    #probalities = voted_probalities/voted_probalities.sum()\n",
    "    #probalities = saferound(probalities, places=4)\n",
    "    return (sku, probalities)\n",
    "\n",
    "predictions = []\n",
    "skus = []\n",
    "with Pool(100) as p:\n",
    "    for data in tqdm(p.imap(voted_shifted_padded_spike, df_test.iterrows()), total=len(df_test)):\n",
    "        sku, probabilities = data\n",
    "        skus.append(sku)\n",
    "        predictions.append(probabilities)\n",
    "        \n",
    "skus = np.array(skus)\n",
    "comparison = skus == df_test['sku'].to_numpy()\n",
    "assert comparison.all()\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/voted_shifted_padded_spike.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d59d0e3-8b2e-4b32-a85a-8e8e64434342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3366558ed0240abaa959e648193db3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shift(arr, num, fill_value=0):\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = fill_value\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = fill_value\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result[:] = arr\n",
    "    return result\n",
    "\n",
    "def voted_shifted_padded_probs(data):\n",
    "    i, row = data\n",
    "    sku = row['sku']\n",
    "    target_stock = row['target_stock']\n",
    "    train_row = df_train_v1.loc[sku]\n",
    "    sold_quantity_series = json.loads(train_row['sold_quantity_series'])\n",
    "\n",
    "    original_len = len(sold_quantity_series)\n",
    "\n",
    "    if original_len < 59:\n",
    "        sold_quantity_series = np.pad(sold_quantity_series, (0, 59-original_len))\n",
    "\n",
    "    #sold_quantity_series = sold_quantity_series[:30]\n",
    "    voted_probalities = np.zeros(30)\n",
    "    for i in range(original_len):\n",
    "        shifted_sum_sold_quantity_series = shift(sold_quantity_series, -i)[:30]\n",
    "\n",
    "        sold_quantity_cumsum = np.cumsum(shifted_sum_sold_quantity_series)\n",
    "        stock_percentage = sold_quantity_cumsum/target_stock\n",
    "        stock_percentage = np.clip(stock_percentage,0,1) \n",
    "        max_consumed = stock_percentage.max()\n",
    "\n",
    "        index_max = np.argmax(stock_percentage == max_consumed)\n",
    "        shifted_probalities = np.eye(30)[index_max]\n",
    "        shifted_probalities *= max_consumed\n",
    "        voted_probalities += shifted_probalities\n",
    "\n",
    "    #index_max = np.argmax(voted_probalities == voted_probalities.max())\n",
    "    #probalities = np.eye(30)[index_max]\n",
    "\n",
    "    if (voted_probalities == np.zeros(30)).all():\n",
    "        voted_probalities[0] = 1\n",
    "    probalities = voted_probalities/voted_probalities.sum()\n",
    "    probalities = saferound(probalities, places=4)\n",
    "    return (sku, probalities)\n",
    "\n",
    "predictions = []\n",
    "skus = []\n",
    "with Pool(100) as p:\n",
    "    for data in tqdm(p.imap(voted_shifted_padded_probs, df_test.iterrows()), total=len(df_test)):\n",
    "        sku, probabilities = data\n",
    "        skus.append(sku)\n",
    "        predictions.append(probabilities)\n",
    "        \n",
    "skus = np.array(skus)\n",
    "comparison = skus == df_test['sku'].to_numpy()\n",
    "assert comparison.all()\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/voted_shifted_padded_probs.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b2e395f-f6b7-4cbf-b55e-b9d0978e7755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b9806ae026d435d9056d2b41f81c0e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/551472 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def shift(arr, num, fill_value=0):\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = fill_value\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = fill_value\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result[:] = arr\n",
    "    return result\n",
    "\n",
    "def gaussian_kernel1d(sigma, length, order=0):\n",
    "    \"\"\"\n",
    "    Computes a 1-D Gaussian convolution kernel.\n",
    "    \"\"\"\n",
    "    if order < 0:\n",
    "        raise ValueError('order must be non-negative')\n",
    "    exponent_range = np.arange(order + 1)\n",
    "    sigma2 = sigma * sigma\n",
    "    x = np.arange(np.floor(-length/2), np.ceil(length/2))\n",
    "    phi_x = np.exp(-0.5 / sigma2 * x ** 2)\n",
    "    phi_x = phi_x / phi_x.sum()\n",
    "    return phi_x\n",
    "\n",
    "\n",
    "def voted_shifted_padded_gaussian_probs(data):\n",
    "    i, row = data\n",
    "    sku = row['sku']\n",
    "    target_stock = row['target_stock']\n",
    "    train_row = df_train_v1.loc[sku]\n",
    "    sold_quantity_series = json.loads(train_row['sold_quantity_series'])\n",
    "\n",
    "    original_len = len(sold_quantity_series)\n",
    "\n",
    "    if original_len < 59:\n",
    "        sold_quantity_series = np.pad(sold_quantity_series, (0, 59-original_len))\n",
    "\n",
    "    #sold_quantity_series = sold_quantity_series[:30]\n",
    "    voted_probalities = np.zeros(30)\n",
    "    for i in range(original_len):\n",
    "        shifted_sum_sold_quantity_series = shift(sold_quantity_series, -i)[:30]\n",
    "\n",
    "        sold_quantity_cumsum = np.cumsum(shifted_sum_sold_quantity_series)\n",
    "        stock_percentage = sold_quantity_cumsum/target_stock\n",
    "        stock_percentage_clipped = np.clip(stock_percentage,0,1) \n",
    "\n",
    "        index_max = np.argmax(stock_percentage_clipped == stock_percentage_clipped.max())\n",
    "        shifted_probalities = np.eye(30)[index_max]\n",
    "        shifted_probalities *= stock_percentage_clipped.max()\n",
    "        #shifted_probalities *= stock_percentage[index_max]\n",
    "        voted_probalities += shifted_probalities\n",
    "\n",
    "    index_max = np.argmax(voted_probalities == voted_probalities.max())\n",
    "    #probalities = np.eye(30)[index_max]\n",
    "\n",
    "    if (voted_probalities == np.zeros(30)).all():\n",
    "        voted_probalities[0] = 1\n",
    "\n",
    "    gaussian_len = 30\n",
    "    sigma = np.sqrt(voted_probalities.std())\n",
    "    sigma = sigma if sigma > 0 else 1\n",
    "    gaussian = gaussian_kernel1d(sigma, gaussian_len)\n",
    "    gaussian_central_point = int(np.floor(gaussian_len/2))\n",
    "    shift_amount = index_max-gaussian_central_point\n",
    "\n",
    "    probalities = voted_probalities*shift(gaussian, shift_amount)\n",
    "\n",
    "    probalities = probalities/probalities.sum()\n",
    "    probalities = saferound(probalities, places=4)\n",
    "    return (sku, probalities)\n",
    "\n",
    "predictions = []\n",
    "skus = []\n",
    "with Pool(100) as p:\n",
    "    for data in tqdm(p.imap(voted_shifted_padded_gaussian_probs, df_test.iterrows()), total=len(df_test)):\n",
    "        sku, probabilities = data\n",
    "        skus.append(sku)\n",
    "        predictions.append(probabilities)\n",
    "        \n",
    "skus = np.array(skus)\n",
    "comparison = skus == df_test['sku'].to_numpy()\n",
    "assert comparison.all()\n",
    "        \n",
    "df_predictions = pd.DataFrame(predictions)\n",
    "df_predictions.to_csv('predictions/voted_shifted_padded_gaussian_probs.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "623e6853-5155-4fbc-bc55-331e31b9b6af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sold_quantity_series [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n",
      "voted_probalities [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5\n",
      " 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5704,\n",
       " 0.3459,\n",
       " 0.0772,\n",
       " 0.0063,\n",
       " 0.0002,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shift(arr, num, fill_value=0):\n",
    "    result = np.empty_like(arr)\n",
    "    if num > 0:\n",
    "        result[:num] = fill_value\n",
    "        result[num:] = arr[:-num]\n",
    "    elif num < 0:\n",
    "        result[num:] = fill_value\n",
    "        result[:num] = arr[-num:]\n",
    "    else:\n",
    "        result[:] = arr\n",
    "    return result\n",
    "\n",
    "def gaussian_kernel1d(sigma, length, order=0):\n",
    "    \"\"\"\n",
    "    Computes a 1-D Gaussian convolution kernel.\n",
    "    \"\"\"\n",
    "    if order < 0:\n",
    "        raise ValueError('order must be non-negative')\n",
    "    exponent_range = np.arange(order + 1)\n",
    "    sigma2 = sigma * sigma\n",
    "    x = np.arange(np.floor(-length/2), np.ceil(length/2))\n",
    "    phi_x = np.exp(-0.5 / sigma2 * x ** 2)\n",
    "    phi_x = phi_x / phi_x.sum()\n",
    "    return phi_x\n",
    "\n",
    "row = df_test.iloc[606]\n",
    "data = (0, row)\n",
    "#test_case\n",
    "\n",
    "i, row = data\n",
    "sku = row['sku']\n",
    "target_stock = row['target_stock']\n",
    "train_row = df_train_v1.loc[sku]\n",
    "sold_quantity_series = json.loads(train_row['sold_quantity_series'])\n",
    "\n",
    "print('sold_quantity_series', sold_quantity_series)\n",
    "\n",
    "original_len = len(sold_quantity_series)\n",
    "\n",
    "if original_len < 59:\n",
    "    sold_quantity_series = np.pad(sold_quantity_series, (0, 59-original_len))\n",
    "\n",
    "#sold_quantity_series = sold_quantity_series[:30]\n",
    "voted_probalities = np.zeros(30)\n",
    "for i in range(original_len):\n",
    "    shifted_sum_sold_quantity_series = shift(sold_quantity_series, -i)[:30]\n",
    "    \n",
    "    sold_quantity_cumsum = np.cumsum(shifted_sum_sold_quantity_series)\n",
    "    stock_percentage = sold_quantity_cumsum/target_stock\n",
    "    stock_percentage_clipped = np.clip(stock_percentage,0,1) \n",
    "\n",
    "    index_max = np.argmax(stock_percentage_clipped == stock_percentage_clipped.max())\n",
    "    shifted_probalities = np.eye(30)[index_max]\n",
    "    shifted_probalities *= stock_percentage_clipped.max()\n",
    "    #shifted_probalities *= stock_percentage[index_max]\n",
    "    voted_probalities += shifted_probalities\n",
    "    \n",
    "index_max = np.argmax(voted_probalities == voted_probalities.max())\n",
    "#probalities = np.eye(30)[index_max]\n",
    "\n",
    "if (voted_probalities == np.zeros(30)).all():\n",
    "    voted_probalities[0] = 1\n",
    "    \n",
    "print('voted_probalities', voted_probalities)\n",
    "\n",
    "gaussian_len = 30\n",
    "sigma = np.sqrt(voted_probalities.std())\n",
    "sigma = sigma if sigma > 0 else 1\n",
    "gaussian = gaussian_kernel1d(sigma, gaussian_len)\n",
    "gaussian_central_point = int(np.floor(gaussian_len/2))\n",
    "shift_amount = index_max-gaussian_central_point\n",
    "\n",
    "probalities = voted_probalities*shift(gaussian, shift_amount)\n",
    "\n",
    "probalities = probalities/probalities.sum()\n",
    "probalities = saferound(probalities, places=4)\n",
    "probalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4aa6f40c-758f-4685-9388-573a143c83cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(voted_probalities.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44b24011-d05d-4e24-992c-7b4d34bff75a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.80774322,  0.30721028,  2.28631206, -1.2033625 ,  1.89920547,\n",
       "        1.19630212,  2.60930263,  2.33853422,  0.68637868,  2.74119347,\n",
       "        0.89575834, -0.63289762,  0.49892918,  1.23123853,  2.10912054,\n",
       "        2.04006513,  1.55623063,  0.73979904,  1.06822032,  1.06394256,\n",
       "        1.40492017,  0.28809414,  1.66671544,  2.31546173,  2.44752861,\n",
       "        0.17956176, -0.55929072,  2.93896606,  1.94720861,  1.25051815])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voted_probalities.mean() + voted_probalities.std() * np.random.standard_normal(size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "66f568ab-78f1-4f88-a535-4e4743b04553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3848f665-77f8-4daf-856f-34292a992539",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 0.33\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.01, 0.08, 0.23, 0.33, 0.23, 0.08, 0.01, 0.  , 0.  , 0.  ,\n",
       "       0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_len = 30\n",
    "gaussian = _gaussian_kernel1d(1.2, gaussian_len)\n",
    "central_point = int(np.floor(gaussian_len/2))\n",
    "print(central_point, gaussian.round(2)[central_point])\n",
    "gaussian.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dca01edc-9050-46fb-aa48-d4769621b928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voted_probalities[index_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "39082116-3b2f-4969-ae06-0d918c4b880a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.39336650e-09, 3.09733232e-07, 7.05864686e-05, 2.89177312e-03,\n",
       "       6.20793976e-02, 3.52314867e-01, 7.63511329e-01, 1.49603355e+00,\n",
       "       8.22242970e-01, 1.24346423e-01, 2.92138342e-02, 1.28523250e-03,\n",
       "       5.64691749e-05, 1.23893293e-06, 1.35734660e-08, 7.42575016e-11,\n",
       "       2.02860044e-13, 2.76731954e-16, 1.88507534e-19, 6.41216552e-23,\n",
       "       1.08914913e-26, 9.23797067e-31, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shift_amount = 15-index_max\n",
    "shift(shift(voted_probalities, shift_amount)*gaussian, -shift_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8069f0da-a913-4b0e-9798-8d0921522e44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "central_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1c5bf3b-f785-476f-bcd6-d61b93550547",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 5, 1, 1, 5, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 2, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[0, 1, 0, 0, 1, (1), 1, 0, 1, 1, 1, 5, 1]\n",
    "[-, 1, 0, 0, 1, 1, (1), 0, 1, 1, 1, 5, 1, 0]\n",
    "[-, -, 0, 0, 1, 1, (1), 0, 1, 1, 1, 5, 1, 0, 0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
